import random
import warnings
import os
import shutil
import numpy as np
import torch
from chatterbox.mtl_tts import ChatterboxMultilingualTTS, SUPPORTED_LANGUAGES
import gradio as gr

# === CONFIGURACI√ìN Y CONSTANTES ===
MAX_CHARS = 10000
CHUNK_SIZE = 400  # Max 400 chars per chunk (model's native limit)

# --- OPTIMIZACI√ìN: Procesamiento paralelo de chunks ---
ENABLE_PARALLEL_CHUNKS = False  # Cambiar a True para activar (experimental)
PARALLEL_WORKERS = 2  # N√∫mero de chunks a procesar en paralelo

# --- GESTI√ìN DE MEMORIA GPU ---
USE_GPU_EMPTY_CACHE = False  # Cambiar a False para desactivar

# --- LIMPIEZA DE CACH√â ---
AUTO_CLEAN_CACHE = True  # Cambiar a False para desactivar limpieza autom√°tica

# --- CARPETA DE SALIDA PERSISTENTE ---
OUTPUT_DIR = os.path.join(os.getcwd(), "outputs")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# === OPTIMIZACIONES DE RENDIMIENTO ===
# Suprimir warnings no cr√≠ticos para limpiar output
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")

# Detectar dispositivo
DEVICE = "cuda" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu")
print(f"üöÄ Running on device: {DEVICE}")

# Optimizaciones espec√≠ficas por dispositivo
if DEVICE == "cuda":
    # === OPTIMIZACIONES CUDA (Windows/Linux) ===
    # TF32 para GPUs Ampere+ (30xx, 40xx) - ~3x m√°s r√°pido
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    # Benchmark para encontrar algoritmos √≥ptimos
    torch.backends.cudnn.benchmark = True
    # Desactivar depuraci√≥n para m√°xima velocidad
    torch.backends.cudnn.deterministic = False
    # Mostrar GPU
    gpu_name = torch.cuda.get_device_name(0)
    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"üéÆ GPU: {gpu_name} ({gpu_mem:.1f} GB)")
elif DEVICE == "mps":
    # === OPTIMIZACIONES MPS (Apple Silicon) ===
    print(f"üçé Apple Silicon (MPS)")

# Liberar memoria GPU al inicio (si est√° habilitado)
if USE_GPU_EMPTY_CACHE:
    if DEVICE == "cuda":
        torch.cuda.empty_cache()
    elif DEVICE == "mps":
        torch.mps.empty_cache()

# --- Global Model Initialization ---
MODEL = None



LANGUAGE_CONFIG = {
    "ar": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ar_f/ar_prompts2.flac",
        "text": "ŸÅŸä ÿßŸÑÿ¥Ÿáÿ± ÿßŸÑŸÖÿßÿ∂Ÿäÿå ŸàÿµŸÑŸÜÿß ÿ•ŸÑŸâ ŸÖÿπŸÑŸÖ ÿ¨ÿØŸäÿØ ÿ®ŸÖŸÑŸäÿßÿ±ŸäŸÜ ŸÖŸÜ ÿßŸÑŸÖÿ¥ÿßŸáÿØÿßÿ™ ÿπŸÑŸâ ŸÇŸÜÿßÿ™ŸÜÿß ÿπŸÑŸâ ŸäŸàÿ™ŸäŸàÿ®."
    },
    "da": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/da_m1.flac",
        "text": "Sidste m√•ned n√•ede vi en ny milep√¶l med to milliarder visninger p√• vores YouTube-kanal."
    },
    "de": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/de_f1.flac",
        "text": "Letzten Monat haben wir einen neuen Meilenstein erreicht: zwei Milliarden Aufrufe auf unserem YouTube-Kanal."
    },
    "el": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/el_m.flac",
        "text": "Œ§ŒøŒΩ œÄŒµœÅŒ±œÉŒºŒ≠ŒΩŒø ŒºŒÆŒΩŒ±, œÜœÑŒ¨œÉŒ±ŒºŒµ œÉŒµ Œ≠ŒΩŒ± ŒΩŒ≠Œø ŒøœÅœåœÉŒ∑ŒºŒø ŒºŒµ Œ¥œçŒø Œ¥ŒπœÉŒµŒ∫Œ±œÑŒøŒºŒºœçœÅŒπŒ± œÄœÅŒøŒ≤ŒøŒªŒ≠œÇ œÉœÑŒø Œ∫Œ±ŒΩŒ¨ŒªŒπ ŒºŒ±œÇ œÉœÑŒø YouTube."
    },
    "en": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/en_f1.flac",
        "text": "Last month, we reached a new milestone with two billion views on our YouTube channel."
    },
    "es": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/es_f1.flac",
        "text": "El mes pasado alcanzamos un nuevo hito: dos mil millones de visualizaciones en nuestro canal de YouTube."
    },
    "fi": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/fi_m.flac",
        "text": "Viime kuussa saavutimme uuden virstanpylv√§√§n kahden miljardin katselukerran kanssa YouTube-kanavallamme."
    },
    "fr": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/fr_f1.flac",
        "text": "Le mois dernier, nous avons atteint un nouveau jalon avec deux milliards de vues sur notre cha√Æne YouTube."
    },
    "he": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/he_m1.flac",
        "text": "◊ë◊ó◊ï◊ì◊© ◊©◊¢◊ë◊® ◊î◊í◊¢◊†◊ï ◊ú◊ê◊ë◊ü ◊ì◊®◊ö ◊ó◊ì◊©◊î ◊¢◊ù ◊©◊†◊ô ◊û◊ô◊ú◊ô◊ê◊®◊ì ◊¶◊§◊ô◊ï◊™ ◊ë◊¢◊®◊ï◊• ◊î◊ô◊ï◊ò◊ô◊ï◊ë ◊©◊ú◊†◊ï."
    },
    "hi": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/hi_f1.flac",
        "text": "‡§™‡§ø‡§õ‡§≤‡•á ‡§Æ‡§π‡•Ä‡§®‡•á ‡§π‡§Æ‡§®‡•á ‡§è‡§ï ‡§®‡§Ø‡§æ ‡§Æ‡•Ä‡§≤ ‡§ï‡§æ ‡§™‡§§‡•ç‡§•‡§∞ ‡§õ‡•Å‡§Ü: ‡§π‡§Æ‡§æ‡§∞‡•á YouTube ‡§ö‡•à‡§®‡§≤ ‡§™‡§∞ ‡§¶‡•ã ‡§Ö‡§∞‡§¨ ‡§µ‡•ç‡§Ø‡•Ç‡§ú‡§º‡•§"
    },
    "it": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/it_m1.flac",
        "text": "Il mese scorso abbiamo raggiunto un nuovo traguardo: due miliardi di visualizzazioni sul nostro canale YouTube."
    },
    "ja": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ja/ja_prompts1.flac",
        "text": "ÂÖàÊúà„ÄÅÁßÅ„Åü„Å°„ÅÆYouTube„ÉÅ„É£„É≥„Éç„É´„Åß‰∫åÂçÅÂÑÑÂõû„ÅÆÂÜçÁîüÂõûÊï∞„Å®„ÅÑ„ÅÜÊñ∞„Åü„Å™„Éû„Ç§„É´„Çπ„Éà„Éº„É≥„Å´Âà∞ÈÅî„Åó„Åæ„Åó„Åü„ÄÇ"
    },
    "ko": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ko_f.flac",
        "text": "ÏßÄÎÇúÎã¨ Ïö∞Î¶¨Îäî Ïú†ÌäúÎ∏å Ï±ÑÎÑêÏóêÏÑú Ïù¥Ïã≠Ïñµ Ï°∞ÌöåÏàòÎùºÎäî ÏÉàÎ°úÏö¥ Ïù¥Ï†ïÌëúÏóê ÎèÑÎã¨ÌñàÏäµÎãàÎã§."
    },
    "ms": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ms_f.flac",
        "text": "Bulan lepas, kami mencapai pencapaian baru dengan dua bilion tontonan di saluran YouTube kami."
    },
    "nl": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/nl_m.flac",
        "text": "Vorige maand bereikten we een nieuwe mijlpaal met twee miljard weergaven op ons YouTube-kanaal."
    },
    "no": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/no_f1.flac",
        "text": "Forrige m√•ned n√•dde vi en ny milep√¶l med to milliarder visninger p√• YouTube-kanalen v√•r."
    },
    "pl": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/pl_m.flac",
        "text": "W zesz≈Çym miesiƒÖcu osiƒÖgnƒôli≈õmy nowy kamie≈Ñ milowy z dwoma miliardami wy≈õwietle≈Ñ na naszym kanale YouTube."
    },
    "pt": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/pt_m1.flac",
        "text": "No m√™s passado, alcan√ß√°mos um novo marco: dois mil milh√µes de visualiza√ß√µes no nosso canal do YouTube."
    },
    "ru": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ru_m.flac",
        "text": "–í –ø—Ä–æ—à–ª–æ–º –º–µ—Å—è—Ü–µ –º—ã –¥–æ—Å—Ç–∏–≥–ª–∏ –Ω–æ–≤–æ–≥–æ —Ä—É–±–µ–∂–∞: –¥–≤–∞ –º–∏–ª–ª–∏–∞—Ä–¥–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –Ω–∞ –Ω–∞—à–µ–º YouTube-–∫–∞–Ω–∞–ª–µ."
    },
    "sv": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/sv_f.flac",
        "text": "F√∂rra m√•naden n√•dde vi en ny milstolpe med tv√• miljarder visningar p√• v√•r YouTube-kanal."
    },
    "sw": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/sw_m.flac",
        "text": "Mwezi uliopita, tulifika hatua mpya ya maoni ya bilioni mbili kweny kituo chetu cha YouTube."
    },
    "tr": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/tr_m.flac",
        "text": "Ge√ßen ay YouTube kanalƒ±mƒ±zda iki milyar g√∂r√ºnt√ºleme ile yeni bir d√∂n√ºm noktasƒ±na ula≈ütƒ±k."
    },
    "zh": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/zh_f2.flac",
        "text": "‰∏ä‰∏™ÊúàÔºåÊàë‰ª¨ËææÂà∞‰∫Ü‰∏Ä‰∏™Êñ∞ÁöÑÈáåÁ®ãÁ¢ë. Êàë‰ª¨ÁöÑYouTubeÈ¢ëÈÅìËßÇÁúãÊ¨°Êï∞ËææÂà∞‰∫Ü‰∫åÂçÅ‰∫øÊ¨°ÔºåËøôÁªùÂØπ‰ª§‰∫∫Èöæ‰ª•ÁΩÆ‰ø°„ÄÇ"
    },
}

# --- UI Helpers ---
def default_audio_for_ui(lang: str) -> str | None:
    return LANGUAGE_CONFIG.get(lang, {}).get("audio")


def default_text_for_ui(lang: str) -> str:
    return LANGUAGE_CONFIG.get(lang, {}).get("text", "")


def get_supported_languages_display() -> str:
    """Generate a formatted display of all supported languages."""
    language_items = []
    for code, name in sorted(SUPPORTED_LANGUAGES.items()):
        language_items.append(f"**{name}** (`{code}`)")
    
    mid = len(language_items) // 2
    line1 = " ‚Ä¢ ".join(language_items[:mid])
    line2 = " ‚Ä¢ ".join(language_items[mid:])
    
    return f"""
### üåç Supported Languages ({len(SUPPORTED_LANGUAGES)} total)
{line1}

{line2}
"""


def open_output_folder():
    """Opens the output folder in the system's file explorer."""
    import subprocess
    import platform
    
    try:
        if platform.system() == "Darwin":  # macOS
            subprocess.run(["open", OUTPUT_DIR], check=True)
        elif platform.system() == "Windows":
            subprocess.run(["explorer", OUTPUT_DIR], check=True)
        else:  # Linux
            subprocess.run(["xdg-open", OUTPUT_DIR], check=True)
        return f"‚úÖ Carpeta abierta: {OUTPUT_DIR}"
    except Exception as e:
        return f"‚ùå Error al abrir carpeta: {e}"


def get_or_load_model():
    """Loads the ChatterboxMultilingualTTS model with optimizations."""
    global MODEL
    if MODEL is None:
        print("Model not loaded, initializing...")
        try:
            MODEL = ChatterboxMultilingualTTS.from_pretrained(DEVICE)
            if hasattr(MODEL, 'to') and str(MODEL.device) != DEVICE:
                MODEL.to(DEVICE)
            
            # === OPTIMIZACI√ìN 1: torch.compile() ===
            # Compilar el modelo para mejor rendimiento (PyTorch 2.0+)
            try:
                print("üî• Compilando modelo con torch.compile()...")
                # Compilar solo en CUDA (MPS no soporta compile a√∫n)
                if DEVICE == "cuda":
                    MODEL = torch.compile(MODEL, mode="reduce-overhead")
                    print("‚úÖ Modelo compilado exitosamente")
                else:
                    print("‚ö†Ô∏è  torch.compile() no disponible en MPS, usando modelo sin compilar")
            except Exception as e:
                print(f"‚ö†Ô∏è  No se pudo compilar el modelo: {e}")
            
            print(f"Model loaded successfully. Internal device: {getattr(MODEL, 'device', 'N/A')}")
        except Exception as e:
            print(f"Error loading model: {e}")
            raise
    return MODEL


# === OPTIMIZACI√ìN 2: Cach√© de embeddings ===
EMBEDDING_CACHE = {}

def get_audio_embedding(audio_path: str, exaggeration: float, model):
    """Obtiene el embedding de audio con cach√© para evitar recomputaci√≥n."""
    cache_key = (audio_path, exaggeration)
    if cache_key in EMBEDDING_CACHE:
        return EMBEDDING_CACHE[cache_key]
    
    print(f"üéôÔ∏è Computando embedding para: {audio_path.split('/')[-1]} (exaggeration: {exaggeration})")
    try:
        # Esto prepara los condicionales dentro del modelo
        model.prepare_conditionals(audio_path, exaggeration=exaggeration)
        # Guardamos una copia del objeto conds (que contiene los tensores del embedding)
        EMBEDDING_CACHE[cache_key] = model.conds
        return model.conds
    except Exception as e:
        print(f"‚ö†Ô∏è Error al preparar embedding: {e}")
        return None


# Attempt to load the model at startup.
try:
    get_or_load_model()
except Exception as e:
    print(f"CRITICAL: Failed to load model on startup. Error: {e}")


def set_seed(seed: int):
    """Sets the random seed for reproducibility."""
    torch.manual_seed(seed)
    if DEVICE == "cuda":
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    np.random.seed(seed)


def split_text_into_chunks(text: str, max_chunk_size: int = CHUNK_SIZE) -> list[str]:
    """
    Split text into chunks at sentence boundaries for natural speech flow.
    """
    import re
    
    # Normalize whitespace
    text = ' '.join(text.split())
    
    if len(text) <= max_chunk_size:
        return [text]
    
    # Split by sentence endings
    sentences = re.split(r'(?<=[.!?„ÄÇÔºÅÔºü])\s+', text)
    
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk) + len(sentence) + 1 <= max_chunk_size:
            current_chunk = f"{current_chunk} {sentence}".strip()
        else:
            if current_chunk:
                chunks.append(current_chunk)
            
            # Handle long sentences
            if len(sentence) > max_chunk_size:
                words = sentence.split()
                current_chunk = ""
                for word in words:
                    if len(current_chunk) + len(word) + 1 <= max_chunk_size:
                        current_chunk = f"{current_chunk} {word}".strip()
                    else:
                        if current_chunk:
                            chunks.append(current_chunk)
                        current_chunk = word
            else:
                current_chunk = sentence
    
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks


@torch.inference_mode()  # M√°s eficiente que no_grad() para inferencia
def generate_audio(
    text_input: str,
    language_id: str,
    audio_prompt_path_input: str = None,
    exaggeration_input: float = 0.5,
    temperature_input: float = 0.8,
    seed_num_input: int = 0,
    cfg_weight_input: float = 0.5,
    repetition_penalty_input: float = 2.0,
    min_p_input: float = 0.05,
    progress=gr.Progress()
) -> str:
    """
    Generate audio for the given text using the TTS model.
    Supports long texts by processing them in chunks (up to 10,000 characters).
    """

    current_model = get_or_load_model()

    if current_model is None:
        raise RuntimeError("TTS model is not loaded.")

    # Validate and truncate text
    text_input = text_input.strip()
    if not text_input:
        raise ValueError("Text input is empty.")
    
    text_input = text_input[:MAX_CHARS]

    if seed_num_input != 0:
        set_seed(int(seed_num_input))

    # Resolve audio prompt and embedding
    chosen_prompt = audio_prompt_path_input or default_audio_for_ui(language_id)
    
    # === USO DE CACH√â DE EMBEDDINGS ===
    if chosen_prompt:
        embedding = get_audio_embedding(chosen_prompt, exaggeration_input, current_model)
        if embedding:
            current_model.conds = embedding
    
    generate_kwargs = {
        "exaggeration": exaggeration_input,
        "temperature": temperature_input,
        "cfg_weight": cfg_weight_input,
        "repetition_penalty": repetition_penalty_input,
        "min_p": min_p_input,
    }
    # NO pasamos audio_prompt_path porque ya seteamos current_model.conds manualmente v√≠a cach√©
    # Si lo pasamos, el modelo volver√≠a a llamar a prepare_conditionals internamente
    # No obstante, si el embedding fall√≥, dejamos que el modelo lo intente cargar normalmente
    if not chosen_prompt:
         # Si no hay prompt, el modelo podr√≠a fallar si no tiene conds
         pass

    # Split text into chunks
    chunks = split_text_into_chunks(text_input)
    total_chunks = len(chunks)
    
    # === RESUMEN AL INICIO ===
    print(f"\n{'='*70}")
    print(f"üìù RESUMEN: {len(text_input):,} caracteres ‚Üí {total_chunks} chunks (m√°x. {CHUNK_SIZE} chars/chunk)")
    print(f"üåê Idioma: {language_id}")
    if chosen_prompt:
        print(f"üé§ Audio de referencia: {chosen_prompt.split('/')[-1]}")
    print(f"{'='*70}\n")

    all_wavs = []
    
    # Progreso inicial
    progress(0, desc=f"üìù Preparando {total_chunks} chunks...")
    
    import time
    start_time = time.time()
    chunk_times = []
    
    # Bucle de generaci√≥n
    for chunk_idx in range(total_chunks):
        chunk_start = time.time()
        chunk_text = chunks[chunk_idx]
        
        # Vista previa del chunk actual
        preview = chunk_text[:45] if len(chunk_text) > 45 else chunk_text
        preview = preview.replace('\n', ' ')
        
        # Progreso en terminal
        if chunk_idx > 0:
            avg_time = sum(chunk_times) / len(chunk_times)
            eta = avg_time * (total_chunks - chunk_idx)
            eta_minutes = int(eta // 60)
            eta_seconds = int(eta % 60)
            print(f"üì¶ [{chunk_idx + 1}/{total_chunks}] '{preview}...' (ETA: {eta_minutes}:{eta_seconds:02d})")
        else:
            print(f"üì¶ [{chunk_idx + 1}/{total_chunks}] '{preview}...'")
        
        # Progreso en Gradio UI
        progress_pct = (chunk_idx + 1) / total_chunks
        progress(progress_pct, desc=f"üéôÔ∏è Chunk {chunk_idx + 1}/{total_chunks}: '{preview[:30]}...'")
        
        wav = current_model.generate(
            chunk_text,
            language_id=language_id,
            **generate_kwargs
        )
        # Normalizar dimensiones del tensor
        wav = wav.squeeze()
        if wav.dim() == 0:
            continue  # Ignorar tensores vac√≠os
        if wav.dim() == 2:
            wav = wav[0]  # Tomar solo el primer canal si es est√©reo
        
        # Guardar como float32 (m√°s r√°pido, convertir a int16 solo al final)
        all_wavs.append(wav.cpu())
        del wav
        
        chunk_times.append(time.time() - chunk_start)

    
    # Concatenate all chunks
    progress(1.0, desc="‚úÖ Concatenando audio...")
    
    if len(all_wavs) == 0:
        raise ValueError("No audio chunks were generated")
    
    # Debug: mostrar formas de chunks
    print(f"\nüìä Debug - Chunks generados: {len(all_wavs)}")
    total_samples = sum(w.shape[-1] for w in all_wavs)
    print(f"   Total samples: {total_samples:,}")
    
    # Concatenar todos los chunks
    final_wav = torch.cat(all_wavs, dim=-1)
    del all_wavs
    
    # === DURACI√ìN FINAL ===
    total_time = time.time() - start_time
    duration = final_wav.shape[-1] / current_model.sr
    
    print(f"\n{'='*70}")
    print(f"‚úÖ GENERACI√ìN COMPLETA")
    print(f"   üìä Chunks procesados: {total_chunks}")
    print(f"   üéµ Total samples: {final_wav.shape[-1]:,}")
    print(f"   ‚è±Ô∏è  Duraci√≥n del audio: {duration:.1f}s ({duration/60:.1f} min)")
    print(f"   ‚ö° Tiempo de generaci√≥n: {total_time:.1f}s ({total_time/60:.1f} min)")
    print(f"   üìà Velocidad: {duration/total_time:.2f}x realtime")
    print(f"{'='*70}\n")
    
    progress(1.0, desc=f"‚úÖ ¬°Completado! Duraci√≥n: {duration:.1f}s | Velocidad: {duration/total_time:.1f}x")
    
    # Guardar audio a archivo WAV temporal
    import tempfile
    import scipy.io.wavfile as wavfile
    
    # Convertir a int16 solo al final
    audio_numpy = final_wav.numpy()
    audio_int16 = (audio_numpy * 32767).astype(np.int16)
    del final_wav, audio_numpy
    
    # Guardar audio en carpeta persistente con timestamp
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = os.path.join(OUTPUT_DIR, f"audio_{timestamp}.wav")
    wavfile.write(output_path, current_model.sr, audio_int16)
    
    print(f"üì§ Audio guardado: {output_path}")
    print(f"   Tama√±o: {len(audio_int16) * 2 / 1e6:.1f} MB")
    
    # Liberar memoria
    del audio_int16
    
    # === LIMPIEZA SELECTIVA DE CACH√â ===
    if AUTO_CLEAN_CACHE:
        print(f"\nüßπ Limpiando cach√© temporal...")
        import subprocess
        import glob
        
        try:
            # 1. Limpiar archivos temporales de Gradio (excepto el audio generado)
            # Usamos glob para encontrar directorios temporales de Gradio
            gradio_temp_dirs = glob.glob("/private/var/folders/*/T/gradio/*")
            for temp_dir in gradio_temp_dirs:
                if os.path.isdir(temp_dir) and temp_file.name not in temp_dir:
                    try:
                        shutil.rmtree(temp_dir, ignore_errors=True)
                    except:
                        pass
            
            # 2. NO BORRAR modelos de Huggingface - solo limpiar lockfiles y temp
            hf_cache = os.path.expanduser('~/.cache/huggingface')
            if os.path.exists(hf_cache):
                # Usamos find via subprocess para eficiencia en √°rboles grandes
                # NO usar capture_output junto con stdout/stderr para evitar conflictos
                subprocess.run(['find', hf_cache, '-name', '*.lock', '-delete'], 
                              check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                subprocess.run(['find', hf_cache, '-type', 'f', '-name', 'tmp*', '-delete'], 
                              check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            # 3. Limpiar solo tarballs de conda (mantener paquetes instalados)
            subprocess.Popen(['conda', 'clean', '--tarballs', '-y'], 
                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            # 4. Limpiar pip cache (en background)
            subprocess.Popen(['pip', 'cache', 'purge'], 
                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            print(f"‚úÖ Cach√© temporal limpiada (modelos preservados)")
        except Exception as e:
            print(f"‚ö†Ô∏è Error limpiando cach√©: {e}")

    
    
    return output_path






# --- Gradio Interface ---
with gr.Blocks() as demo:
    gr.Markdown(
        """
        # üéôÔ∏è Chatterbox Studio
        **Professional Multilingual Text-to-Speech Engine**
        
        Generate high-quality multilingual speech from text with reference audio styling.
        Supports up to **10,000 characters** with automatic chunk processing.
        """
    )
    
    gr.Markdown(get_supported_languages_display())
    
    with gr.Row():
        with gr.Column(scale=1):
            initial_lang = "es"
            
            language_id = gr.Dropdown(
                choices=list(ChatterboxMultilingualTTS.get_supported_languages().keys()),
                value=initial_lang,
                label="üåê Language",
                info="Select the language for synthesis"
            )
            
            ref_wav = gr.Audio(
                sources=["upload", "microphone"],
                type="filepath",
                label="üé§ Reference Audio (Optional)",
                value=default_audio_for_ui(initial_lang)
            )
            
            gr.Markdown(
                "üí° **Tip**: Match reference audio language with selected language for best results.",
                elem_classes=["audio-note"]
            )
            
            exaggeration = gr.Slider(
                0.25, 2, step=0.05, 
                label="üé≠ Exaggeration", 
                value=0.5,
                info="Neutral = 0.5"
            )
            
            cfg_weight = gr.Slider(
                0.2, 1, step=0.05, 
                label="‚ö° CFG/Pace", 
                value=0.5
            )

            with gr.Accordion("‚öôÔ∏è Advanced Options", open=False):
                seed_num = gr.Number(value=0, label="Random Seed (0 = random)")
                temp = gr.Slider(0.05, 5, step=0.05, label="Temperature", value=0.8)
                repetition_penalty = gr.Slider(1.0, 10.0, step=0.1, label="Repetition Penalty", value=2.0)
                min_p = gr.Slider(0.01, 0.5, step=0.01, label="Min P", value=0.05)

        with gr.Column(scale=2):
            text = gr.Textbox(
                value=default_text_for_ui(initial_lang),
                label=f"üìù Text to Synthesize (max {MAX_CHARS:,} characters)",
                lines=10,
                max_lines=20
            )
            
            run_btn = gr.Button("üöÄ Generate Audio", variant="primary", size="lg")
            
            open_folder_btn = gr.Button("ÔøΩ Open Output Folder", size="sm")
            
            audio_output = gr.Audio(label="ÔøΩ Generated Audio")
            
            open_folder_btn.click(
                fn=open_output_folder,
                inputs=[],
                outputs=[]
            )

    def on_language_change(lang, current_ref, current_text):
        return default_audio_for_ui(lang), default_text_for_ui(lang)

    language_id.change(
        fn=on_language_change,
        inputs=[language_id, ref_wav, text],
        outputs=[ref_wav, text],
        show_progress=False
    )

    run_btn.click(
        fn=generate_audio,
        inputs=[
            text,
            language_id,
            ref_wav,
            exaggeration,
            temp,
            seed_num,
            cfg_weight,
            repetition_penalty,
            min_p,
        ],
        outputs=[audio_output],
    )

if __name__ == "__main__":
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860
    )
